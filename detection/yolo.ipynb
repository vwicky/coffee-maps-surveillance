{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1191725",
   "metadata": {},
   "source": [
    "### YOLOv8\n",
    "A computer vision model architecture for detection, classification, segmentation, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95101f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ModelDependencyMissing: Your `inference` configuration does not support Qwen2.5-VL model. Use pip install 'inference[transformers]' to install missing requirements.To suppress this warning, set QWEN_2_5_ENABLED to False.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[sam]' to install missing requirements.To suppress this warning, set CORE_MODEL_SAM_ENABLED to False.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support SAM2 model. Use pip install 'inference[sam]' to install missing requirements.To suppress this warning, set CORE_MODEL_SAM2_ENABLED to False.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support CLIP model. Use pip install 'inference[clip]' to install missing requirements.To suppress this warning, set CORE_MODEL_CLIP_ENABLED to False.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support Gaze Detection model. Use pip install 'inference[gaze]' to install missing requirements.To suppress this warning, set CORE_MODEL_GAZE_ENABLED to False.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support SmolVLM2.Use pip install 'inference[transformers]' to install missing requirements.To suppress this warning, set SMOLVLM2_ENABLED to False.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support GroundingDINO model. Use pip install 'inference[grounding-dino]' to install missing requirements.To suppress this warning, set CORE_MODEL_GROUNDINGDINO_ENABLED to False.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support YoloWorld model. Use pip install 'inference[yolo-world]' to install missing requirements.To suppress this warning, set CORE_MODEL_YOLO_WORLD_ENABLED to False.\n",
      "ModelDependencyMissing: Your `inference` configuration does not support Perception Encoder.Use pip install 'inference[transformers]' to install missing requirements.To suppress this warning, set CORE_MODEL_PE_ENABLED to False.\n"
     ]
    }
   ],
   "source": [
    "from inference import InferencePipeline\n",
    "from inference.core.interfaces.stream.sinks import render_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aaac376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "UserWarning: Specified provider 'OpenVINOExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "UserWarning: Specified provider 'CoreMLExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m InferencePipeline\u001b[38;5;241m.\u001b[39minit(\n\u001b[0;32m      2\u001b[0m     model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov8n-640\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     video_reference\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Angelika/Videos/Captures/coffee.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     on_prediction\u001b[38;5;241m=\u001b[39mrender_boxes\n\u001b[0;32m      5\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mПрограма завершилася після обробки всього відео.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\core\\interfaces\\stream\\inference_pipeline.py:861\u001b[0m, in \u001b[0;36mInferencePipeline.start\u001b[1;34m(self, use_main_thread)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_pipeline_start()\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_main_thread:\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_inference_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatching_thread \u001b[38;5;241m=\u001b[39m Thread(target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_inference_results)\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inference\\core\\interfaces\\stream\\inference_pipeline.py:952\u001b[0m, in \u001b[0;36mInferencePipeline._dispatch_inference_results\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dispatch_inference_results\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    950\u001b[0m         inference_results: Optional[\n\u001b[0;32m    951\u001b[0m             Tuple[List[AnyPrediction], List[VideoFrame]]\n\u001b[1;32m--> 952\u001b[0m         ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictions_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inference_results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictions_queue\u001b[38;5;241m.\u001b[39mtask_done()\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[1;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline = InferencePipeline.init(\n",
    "    model_id=\"yolov8n-640\",\n",
    "    video_reference=\"C:/Users/Angelika/Videos/Captures/coffee.mp4\",\n",
    "    on_prediction=render_boxes\n",
    ")\n",
    "\n",
    "pipeline.start()\n",
    "pipeline.join()\n",
    "\n",
    "print(\"Програма завершилася після обробки всього відео.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd2fe70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "video 1/1 (frame 1/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 1 dining table, 2316.5ms\n",
      "video 1/1 (frame 2/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 1 dining table, 485.2ms\n",
      "video 1/1 (frame 3/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 11 persons, 1 dining table, 560.0ms\n",
      "video 1/1 (frame 4/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 11 persons, 1 dining table, 591.5ms\n",
      "video 1/1 (frame 5/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 11 persons, 1 dining table, 562.5ms\n",
      "video 1/1 (frame 6/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 326.9ms\n",
      "video 1/1 (frame 7/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 324.8ms\n",
      "video 1/1 (frame 8/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 370.7ms\n",
      "video 1/1 (frame 9/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 414.5ms\n",
      "video 1/1 (frame 10/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 452.5ms\n",
      "video 1/1 (frame 11/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 11 persons, 380.0ms\n",
      "video 1/1 (frame 12/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 401.1ms\n",
      "video 1/1 (frame 13/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 11 persons, 418.2ms\n",
      "video 1/1 (frame 14/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 1608.0ms\n",
      "video 1/1 (frame 15/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 357.4ms\n",
      "video 1/1 (frame 16/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 375.0ms\n",
      "video 1/1 (frame 17/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 370.8ms\n",
      "video 1/1 (frame 18/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 680.4ms\n",
      "video 1/1 (frame 19/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 8 persons, 376.1ms\n",
      "video 1/1 (frame 20/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 8 persons, 320.7ms\n",
      "video 1/1 (frame 21/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 319.2ms\n",
      "video 1/1 (frame 22/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 372.8ms\n",
      "video 1/1 (frame 23/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 386.6ms\n",
      "video 1/1 (frame 24/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 381.5ms\n",
      "video 1/1 (frame 25/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 373.7ms\n",
      "video 1/1 (frame 26/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 393.2ms\n",
      "video 1/1 (frame 27/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 658.7ms\n",
      "video 1/1 (frame 28/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 385.8ms\n",
      "video 1/1 (frame 29/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 11 persons, 509.9ms\n",
      "video 1/1 (frame 30/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 11 persons, 442.4ms\n",
      "video 1/1 (frame 31/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 541.8ms\n",
      "video 1/1 (frame 32/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 567.8ms\n",
      "video 1/1 (frame 33/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 559.5ms\n",
      "video 1/1 (frame 34/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 535.7ms\n",
      "video 1/1 (frame 35/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 11 persons, 1 potted plant, 529.5ms\n",
      "video 1/1 (frame 36/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 645.3ms\n",
      "video 1/1 (frame 37/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 404.3ms\n",
      "video 1/1 (frame 38/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 541.0ms\n",
      "video 1/1 (frame 39/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 11 persons, 463.7ms\n",
      "video 1/1 (frame 40/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 320.8ms\n",
      "video 1/1 (frame 41/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 342.4ms\n",
      "video 1/1 (frame 42/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 319.7ms\n",
      "video 1/1 (frame 43/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 14 persons, 401.4ms\n",
      "video 1/1 (frame 44/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 13 persons, 378.6ms\n",
      "video 1/1 (frame 45/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 13 persons, 478.0ms\n",
      "video 1/1 (frame 46/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 13 persons, 1 chair, 412.1ms\n",
      "video 1/1 (frame 47/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 11 persons, 1 chair, 375.0ms\n",
      "video 1/1 (frame 48/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 381.5ms\n",
      "video 1/1 (frame 49/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 1 banana, 449.5ms\n",
      "video 1/1 (frame 50/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 2 bananas, 1 chair, 453.9ms\n",
      "video 1/1 (frame 51/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 2 bananas, 402.0ms\n",
      "video 1/1 (frame 52/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 452.0ms\n",
      "video 1/1 (frame 53/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 1 parking meter, 471.7ms\n",
      "video 1/1 (frame 54/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 455.3ms\n",
      "video 1/1 (frame 55/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 8 persons, 481.3ms\n",
      "video 1/1 (frame 56/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 390.0ms\n",
      "video 1/1 (frame 57/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 8 persons, 491.5ms\n",
      "video 1/1 (frame 58/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 399.6ms\n",
      "video 1/1 (frame 59/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 402.1ms\n",
      "video 1/1 (frame 60/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 397.7ms\n",
      "video 1/1 (frame 61/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 8 persons, 410.9ms\n",
      "video 1/1 (frame 62/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 8 persons, 505.2ms\n",
      "video 1/1 (frame 63/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 842.2ms\n",
      "video 1/1 (frame 64/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 11 persons, 735.8ms\n",
      "video 1/1 (frame 65/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 478.8ms\n",
      "video 1/1 (frame 66/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 380.6ms\n",
      "video 1/1 (frame 67/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 11 persons, 418.2ms\n",
      "video 1/1 (frame 68/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 13 persons, 415.2ms\n",
      "video 1/1 (frame 69/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 12 persons, 661.1ms\n",
      "video 1/1 (frame 70/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 12 persons, 385.7ms\n",
      "video 1/1 (frame 71/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 12 persons, 393.8ms\n",
      "video 1/1 (frame 72/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 12 persons, 334.5ms\n",
      "video 1/1 (frame 73/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 12 persons, 1 suitcase, 383.1ms\n",
      "video 1/1 (frame 74/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 12 persons, 552.1ms\n",
      "video 1/1 (frame 75/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 346.0ms\n",
      "video 1/1 (frame 76/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 432.6ms\n",
      "video 1/1 (frame 77/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 363.9ms\n",
      "video 1/1 (frame 78/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 1 suitcase, 331.0ms\n",
      "video 1/1 (frame 79/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 331.6ms\n",
      "video 1/1 (frame 80/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 336.4ms\n",
      "video 1/1 (frame 81/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 331.0ms\n",
      "video 1/1 (frame 82/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 319.0ms\n",
      "video 1/1 (frame 83/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 11 persons, 331.5ms\n",
      "video 1/1 (frame 84/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 320.9ms\n",
      "video 1/1 (frame 85/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 347.0ms\n",
      "video 1/1 (frame 86/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 387.7ms\n",
      "video 1/1 (frame 87/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 467.5ms\n",
      "video 1/1 (frame 88/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 8 persons, 426.0ms\n",
      "video 1/1 (frame 89/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 357.0ms\n",
      "video 1/1 (frame 90/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 339.6ms\n",
      "video 1/1 (frame 91/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 337.0ms\n",
      "video 1/1 (frame 92/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 1 chair, 390.0ms\n",
      "video 1/1 (frame 93/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 400.8ms\n",
      "video 1/1 (frame 94/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 1 chair, 496.2ms\n",
      "video 1/1 (frame 95/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 9 persons, 1 chair, 363.0ms\n",
      "video 1/1 (frame 96/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 1 chair, 625.8ms\n",
      "video 1/1 (frame 97/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 12 persons, 1 chair, 382.3ms\n",
      "video 1/1 (frame 98/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 12 persons, 1 chair, 466.9ms\n",
      "video 1/1 (frame 99/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 1 chair, 741.3ms\n",
      "video 1/1 (frame 100/120) C:\\Users\\Angelika\\Videos\\Captures\\coffee.mp4: 192x320 10 persons, 628.0ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Angelika/Videos/Captures/coffee.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m results \u001b[38;5;241m=\u001b[39m model(video_path, imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m320\u001b[39m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# imgsz=320, 640, 1280\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# кадр із боксами\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1068\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:57\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[0;32m     56\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 57\u001b[0m                 response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:336\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 336\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    338\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:184\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    179\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    180\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    183\u001b[0m )\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:637\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed, **kwargs)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 637\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:139\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:157\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:180\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 180\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    181\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:318\u001b[0m, in \u001b[0;36mC2f.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m    317\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 318\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:318\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m    317\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 318\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:495\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    494\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply bottleneck with optional shortcut connection.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x))\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:93\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     84\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:432\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2379\u001b[0m, in \u001b[0;36msilu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   2377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[0;32m   2378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 2379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8s.pt\")  # s, m, l, x \n",
    "\n",
    "# coffee_shop, coffee, cats\n",
    "video_path = \"C:/Users/Angelika/Videos/Captures/coffee.mp4\"\n",
    "\n",
    "results = model(video_path, imgsz=320, stream=True)  # imgsz=320, 640, 1280\n",
    "for r in results:\n",
    "    frame = r.plot()  # кадр із боксами\n",
    "    frame = cv2.resize(frame, (1068, 600))  \n",
    "    cv2.imshow(\"Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7c6ced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "video 1/1 (frame 1/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 12 cats, 269.6ms\n",
      "video 1/1 (frame 2/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 14 cats, 255.7ms\n",
      "video 1/1 (frame 3/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 9 cats, 1 dog, 351.3ms\n",
      "video 1/1 (frame 4/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 11 cats, 310.8ms\n",
      "video 1/1 (frame 5/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 16 cats, 235.3ms\n",
      "video 1/1 (frame 6/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 9 cats, 228.6ms\n",
      "video 1/1 (frame 7/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 10 cats, 225.8ms\n",
      "video 1/1 (frame 8/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 1 car, 9 cats, 302.9ms\n",
      "video 1/1 (frame 9/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 8 cats, 301.1ms\n",
      "video 1/1 (frame 10/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 1 person, 1 bench, 9 cats, 329.2ms\n",
      "video 1/1 (frame 11/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 10 cats, 1 toilet, 268.6ms\n",
      "video 1/1 (frame 12/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 9 cats, 1 toilet, 239.9ms\n",
      "video 1/1 (frame 13/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 10 cats, 1 potted plant, 289.1ms\n",
      "video 1/1 (frame 14/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 10 cats, 251.0ms\n",
      "video 1/1 (frame 15/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 10 cats, 231.7ms\n",
      "video 1/1 (frame 16/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 9 cats, 330.4ms\n",
      "video 1/1 (frame 17/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 10 cats, 2 dogs, 222.6ms\n",
      "video 1/1 (frame 18/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 11 cats, 217.8ms\n",
      "video 1/1 (frame 19/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 11 cats, 220.2ms\n",
      "video 1/1 (frame 20/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 8 cats, 215.9ms\n",
      "video 1/1 (frame 21/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 1 bird, 13 cats, 217.4ms\n",
      "video 1/1 (frame 22/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 11 cats, 221.1ms\n",
      "video 1/1 (frame 23/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 11 cats, 228.9ms\n",
      "video 1/1 (frame 24/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 11 cats, 222.6ms\n",
      "video 1/1 (frame 25/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 10 cats, 218.5ms\n",
      "video 1/1 (frame 26/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 9 cats, 217.4ms\n",
      "video 1/1 (frame 27/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 12 cats, 217.7ms\n",
      "video 1/1 (frame 28/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 9 cats, 1 dog, 216.8ms\n",
      "video 1/1 (frame 29/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 9 cats, 1 dog, 225.3ms\n",
      "video 1/1 (frame 30/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 1 bird, 11 cats, 217.0ms\n",
      "video 1/1 (frame 31/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 12 cats, 228.0ms\n",
      "video 1/1 (frame 32/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 1 bench, 9 cats, 228.0ms\n",
      "video 1/1 (frame 33/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 13 cats, 243.1ms\n",
      "video 1/1 (frame 34/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 10 cats, 220.9ms\n",
      "video 1/1 (frame 35/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 10 cats, 228.7ms\n",
      "video 1/1 (frame 36/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 1 bird, 10 cats, 312.2ms\n",
      "video 1/1 (frame 37/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 1 bird, 12 cats, 329.7ms\n",
      "video 1/1 (frame 38/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 11 cats, 221.8ms\n",
      "video 1/1 (frame 39/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 10 cats, 225.4ms\n",
      "video 1/1 (frame 40/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 9 cats, 232.4ms\n",
      "video 1/1 (frame 41/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 1 bird, 8 cats, 1 dog, 257.0ms\n",
      "video 1/1 (frame 42/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 13 cats, 237.4ms\n",
      "video 1/1 (frame 43/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 11 cats, 1 dog, 372.0ms\n",
      "video 1/1 (frame 44/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 11 cats, 500.7ms\n",
      "video 1/1 (frame 45/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 1 bench, 1 bird, 10 cats, 282.3ms\n",
      "video 1/1 (frame 46/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 1 bird, 10 cats, 348.1ms\n",
      "video 1/1 (frame 47/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 1 bench, 1 bird, 7 cats, 357.1ms\n",
      "video 1/1 (frame 48/163) C:\\Users\\Angelika\\Videos\\Captures\\cats.mp4: 384x640 1 bench, 10 cats, 358.6ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m results \u001b[38;5;241m=\u001b[39m model(video_path, imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m640\u001b[39m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# imgsz=320, 640, 1280\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m---> 11\u001b[0m     frame \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39morig_img\u001b[38;5;241m.\u001b[39mcopy()  \u001b[38;5;66;03m# кадр без підписів\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m box \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39mboxes:  \u001b[38;5;66;03m# перебір усіх боксов\u001b[39;00m\n\u001b[0;32m     13\u001b[0m         x1, y1, x2, y2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, box\u001b[38;5;241m.\u001b[39mxyxy[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8s.pt\")  # s, m, l, x \n",
    "\n",
    "video_path = \"C:/Users/Angelika/Videos/Captures/cats.mp4\"\n",
    "\n",
    "results = model(video_path, imgsz=640, stream=True)  # imgsz=320, 640, 1280\n",
    "\n",
    "for r in results:\n",
    "    frame = r.orig_img.copy()  # кадр без підписів\n",
    "    for box in r.boxes:  # перебір усіх боксов\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), thickness=5)  # товщина=1\n",
    "    frame = cv2.resize(frame, (1068, 600))  \n",
    "    cv2.imshow(\"Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a917eb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 tv, 143.2ms\n",
      "Speed: 33.8ms preprocess, 143.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 117.3ms\n",
      "Speed: 3.0ms preprocess, 117.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 117.8ms\n",
      "Speed: 3.2ms preprocess, 117.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 126.3ms\n",
      "Speed: 18.3ms preprocess, 126.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 212.3ms\n",
      "Speed: 4.9ms preprocess, 212.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 115.8ms\n",
      "Speed: 2.9ms preprocess, 115.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 134.8ms\n",
      "Speed: 3.7ms preprocess, 134.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 126.9ms\n",
      "Speed: 3.2ms preprocess, 126.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 161.9ms\n",
      "Speed: 4.9ms preprocess, 161.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 156.7ms\n",
      "Speed: 35.2ms preprocess, 156.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 107.8ms\n",
      "Speed: 4.4ms preprocess, 107.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 115.6ms\n",
      "Speed: 3.7ms preprocess, 115.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 121.8ms\n",
      "Speed: 3.1ms preprocess, 121.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 160.0ms\n",
      "Speed: 3.2ms preprocess, 160.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 132.6ms\n",
      "Speed: 5.4ms preprocess, 132.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 124.9ms\n",
      "Speed: 3.4ms preprocess, 124.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 191.2ms\n",
      "Speed: 3.1ms preprocess, 191.2ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 118.9ms\n",
      "Speed: 3.2ms preprocess, 118.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 219.4ms\n",
      "Speed: 8.0ms preprocess, 219.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 170.8ms\n",
      "Speed: 23.4ms preprocess, 170.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 214.2ms\n",
      "Speed: 12.2ms preprocess, 214.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 252.9ms\n",
      "Speed: 5.4ms preprocess, 252.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 126.3ms\n",
      "Speed: 4.3ms preprocess, 126.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 tv, 303.2ms\n",
      "Speed: 4.1ms preprocess, 303.2ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 tv, 133.9ms\n",
      "Speed: 2.8ms preprocess, 133.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 127.7ms\n",
      "Speed: 3.2ms preprocess, 127.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 122.5ms\n",
      "Speed: 6.6ms preprocess, 122.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 123.7ms\n",
      "Speed: 4.6ms preprocess, 123.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 120.0ms\n",
      "Speed: 3.3ms preprocess, 120.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 129.0ms\n",
      "Speed: 3.3ms preprocess, 129.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 129.9ms\n",
      "Speed: 2.8ms preprocess, 129.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 156.2ms\n",
      "Speed: 4.5ms preprocess, 156.2ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 130.9ms\n",
      "Speed: 3.4ms preprocess, 130.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 tv, 153.6ms\n",
      "Speed: 5.0ms preprocess, 153.6ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 151.9ms\n",
      "Speed: 8.1ms preprocess, 151.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 247.6ms\n",
      "Speed: 3.0ms preprocess, 247.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 tv, 171.0ms\n",
      "Speed: 6.7ms preprocess, 171.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 tv, 214.3ms\n",
      "Speed: 3.2ms preprocess, 214.3ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 chair, 138.3ms\n",
      "Speed: 5.0ms preprocess, 138.3ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 123.4ms\n",
      "Speed: 4.9ms preprocess, 123.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 128.6ms\n",
      "Speed: 5.2ms preprocess, 128.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 134.0ms\n",
      "Speed: 3.7ms preprocess, 134.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 174.6ms\n",
      "Speed: 91.6ms preprocess, 174.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 tv, 1 laptop, 148.9ms\n",
      "Speed: 6.8ms preprocess, 148.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 tv, 131.7ms\n",
      "Speed: 3.1ms preprocess, 131.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 laptop, 153.2ms\n",
      "Speed: 5.6ms preprocess, 153.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 185.0ms\n",
      "Speed: 6.0ms preprocess, 185.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 chair, 1 laptop, 168.1ms\n",
      "Speed: 6.0ms preprocess, 168.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 laptop, 188.3ms\n",
      "Speed: 15.6ms preprocess, 188.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 147.3ms\n",
      "Speed: 3.9ms preprocess, 147.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 127.6ms\n",
      "Speed: 5.2ms preprocess, 127.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 122.2ms\n",
      "Speed: 3.4ms preprocess, 122.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 128.5ms\n",
      "Speed: 3.3ms preprocess, 128.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov8n.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     screenshot \u001b[38;5;241m=\u001b[39m \u001b[43mpyautogui\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreenshot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     frame \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(screenshot)\n\u001b[0;32m     11\u001b[0m     frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR)  \u001b[38;5;66;03m# змінюємо кольори\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyscreeze\\__init__.py:538\u001b[0m, in \u001b[0;36m_screenshot_win32\u001b[1;34m(imageFilename, region, allScreens)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;124;03mTODO\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;66;03m# TODO - Use the winapi to get a screenshot, and compare performance with ImageGrab.grab()\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;66;03m# https://stackoverflow.com/a/3586280/1893164\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43mImageGrab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_screens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallScreens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m region \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(region) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregion argument must be a tuple of four ints\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageGrab.py:66\u001b[0m, in \u001b[0;36mgrab\u001b[1;34m(bbox, include_layered_windows, all_screens, xdisplay, window)\u001b[0m\n\u001b[0;32m     60\u001b[0m     all_screens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     61\u001b[0m offset, size, data \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mgrabscreen_win32(\n\u001b[0;32m     62\u001b[0m     include_layered_windows,\n\u001b[0;32m     63\u001b[0m     all_screens,\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mint\u001b[39m(window) \u001b[38;5;28;01mif\u001b[39;00m window \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     65\u001b[0m )\n\u001b[1;32m---> 66\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrombytes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# RGB, 32-bit line padding, origin lower left corner\u001b[39;49;00m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBGR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox:\n\u001b[0;32m     77\u001b[0m     x0, y0 \u001b[38;5;241m=\u001b[39m offset\n",
      "File \u001b[1;32mc:\\Users\\Angelika\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:3120\u001b[0m, in \u001b[0;36mfrombytes\u001b[1;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m   3116\u001b[0m             color \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpalette\u001b[38;5;241m.\u001b[39mgetcolor(color_ints)\n\u001b[0;32m   3117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im\u001b[38;5;241m.\u001b[39m_new(core\u001b[38;5;241m.\u001b[39mfill(mode, size, color))\n\u001b[1;32m-> 3120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrombytes\u001b[39m(\n\u001b[0;32m   3121\u001b[0m     mode: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   3122\u001b[0m     size: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m   3123\u001b[0m     data: \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mbytearray\u001b[39m \u001b[38;5;241m|\u001b[39m SupportsArrayInterface,\n\u001b[0;32m   3124\u001b[0m     decoder_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3125\u001b[0m     \u001b[38;5;241m*\u001b[39margs: Any,\n\u001b[0;32m   3126\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image:\n\u001b[0;32m   3127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3128\u001b[0m \u001b[38;5;124;03m    Creates a copy of an image memory from pixel data in a buffer.\u001b[39;00m\n\u001b[0;32m   3129\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3147\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m   3148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3150\u001b[0m     _check_size(size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "while True:\n",
    "    screenshot = pyautogui.screenshot()\n",
    "    frame = np.array(screenshot)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # змінюємо кольори\n",
    "\n",
    "    results = model(frame, imgsz=640, stream=False)  # обробка одного кадру\n",
    "    for r in results:\n",
    "        frame = r.plot()  # малюємо бокси\n",
    "    \n",
    "    cv2.imshow(\"Screen Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "735e7df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 8 persons, 121.7ms\n",
      "Speed: 29.0ms preprocess, 121.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 129.3ms\n",
      "Speed: 3.3ms preprocess, 129.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 103.7ms\n",
      "Speed: 2.9ms preprocess, 103.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 101.9ms\n",
      "Speed: 2.9ms preprocess, 101.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 100.0ms\n",
      "Speed: 2.8ms preprocess, 100.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 105.8ms\n",
      "Speed: 2.9ms preprocess, 105.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 105.0ms\n",
      "Speed: 2.9ms preprocess, 105.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 1 laptop, 103.4ms\n",
      "Speed: 2.9ms preprocess, 103.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 104.9ms\n",
      "Speed: 2.9ms preprocess, 104.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 107.9ms\n",
      "Speed: 3.1ms preprocess, 107.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 101.3ms\n",
      "Speed: 2.9ms preprocess, 101.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 99.8ms\n",
      "Speed: 2.9ms preprocess, 99.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 103.0ms\n",
      "Speed: 3.3ms preprocess, 103.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "frame_idx = 0\n",
    "process_every_n = 2   # обробляти кожен 3-й кадр\n",
    "\n",
    "while True:\n",
    "    screenshot = pyautogui.screenshot()\n",
    "    frame = np.array(screenshot)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # змінюємо кольори\n",
    "\n",
    "    frame_idx += 1\n",
    "    if frame_idx % process_every_n == 0:  \n",
    "        results = model(frame, imgsz=640, stream=False)  # обробка кадру\n",
    "        for r in results: \n",
    "            frame = r.plot()  # малюємо бокси\n",
    "\n",
    "    # показуємо останній кадр (навіть якщо пропущений)\n",
    "    cv2.imshow(\"Screen Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b6073037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 9 persons, 1 cup, 175.1ms\n",
      "Speed: 34.4ms preprocess, 175.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 cup, 168.7ms\n",
      "Speed: 6.0ms preprocess, 168.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 cup, 197.1ms\n",
      "Speed: 4.0ms preprocess, 197.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 cups, 200.2ms\n",
      "Speed: 4.1ms preprocess, 200.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 197.2ms\n",
      "Speed: 3.7ms preprocess, 197.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 1 cup, 176.3ms\n",
      "Speed: 6.1ms preprocess, 176.3ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 151.7ms\n",
      "Speed: 7.6ms preprocess, 151.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bottle, 1 cup, 154.2ms\n",
      "Speed: 7.5ms preprocess, 154.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 1 chair, 152.9ms\n",
      "Speed: 3.1ms preprocess, 152.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 165.1ms\n",
      "Speed: 64.3ms preprocess, 165.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 146.8ms\n",
      "Speed: 4.5ms preprocess, 146.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 153.3ms\n",
      "Speed: 4.0ms preprocess, 153.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 146.3ms\n",
      "Speed: 5.2ms preprocess, 146.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bottle, 2 cups, 149.2ms\n",
      "Speed: 3.1ms preprocess, 149.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 147.0ms\n",
      "Speed: 3.4ms preprocess, 147.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 cups, 150.9ms\n",
      "Speed: 3.3ms preprocess, 150.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bottle, 1 cup, 1 potted plant, 147.6ms\n",
      "Speed: 4.6ms preprocess, 147.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bottle, 2 cups, 146.5ms\n",
      "Speed: 3.7ms preprocess, 146.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3 cups, 147.1ms\n",
      "Speed: 4.1ms preprocess, 147.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 157.3ms\n",
      "Speed: 8.8ms preprocess, 157.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 bottle, 2 cups, 142.0ms\n",
      "Speed: 3.4ms preprocess, 142.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 bottle, 2 cups, 143.4ms\n",
      "Speed: 4.4ms preprocess, 143.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 cup, 144.0ms\n",
      "Speed: 3.1ms preprocess, 144.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 cup, 145.7ms\n",
      "Speed: 3.4ms preprocess, 145.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 cup, 150.7ms\n",
      "Speed: 3.6ms preprocess, 150.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 cup, 141.5ms\n",
      "Speed: 3.5ms preprocess, 141.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 cup, 141.3ms\n",
      "Speed: 6.2ms preprocess, 141.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 cup, 148.0ms\n",
      "Speed: 3.5ms preprocess, 148.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 bottle, 1 cup, 140.7ms\n",
      "Speed: 5.9ms preprocess, 140.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bottle, 148.5ms\n",
      "Speed: 7.6ms preprocess, 148.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 cup, 1 chair, 149.3ms\n",
      "Speed: 5.7ms preprocess, 149.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bottle, 144.0ms\n",
      "Speed: 3.4ms preprocess, 144.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bottle, 1 cup, 1 chair, 146.2ms\n",
      "Speed: 6.9ms preprocess, 146.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bottle, 1 cup, 1 chair, 151.6ms\n",
      "Speed: 6.1ms preprocess, 151.6ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 1 cup, 145.1ms\n",
      "Speed: 4.6ms preprocess, 145.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 bottle, 1 cup, 263.1ms\n",
      "Speed: 3.7ms preprocess, 263.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bottle, 1 cup, 148.6ms\n",
      "Speed: 5.1ms preprocess, 148.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bottle, 156.1ms\n",
      "Speed: 3.8ms preprocess, 156.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# моя любіма\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "from ultralytics import YOLO\n",
    "import math\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# object classes\n",
    "classNames = [\"person\", \"cat\", \"dog\"]\n",
    "# if we need  \"cup\", \"bottle\", \"laptop\", \"chair\", \"diningtable\", \"cell phone\", \"potted plant\"\n",
    "\n",
    "# DeepSORT трекер\n",
    "tracker = DeepSort(max_age=30)\n",
    "\n",
    "frame_idx = 0\n",
    "process_every_n = 3   # обробляти кожен 3-й кадр\n",
    "last_results = None   # сюди зберігаємо останні результати\n",
    "\n",
    "# video\n",
    "video_path = \"C:/Users/Angelika/Videos/Captures/coffee_shop.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# налаштування запису відео \n",
    "frame_width, frame_height = pyautogui.size()  \n",
    "out = cv2.VideoWriter(\n",
    "    'screen_detection.mp4',                 \n",
    "    cv2.VideoWriter_fourcc(*'mp4v'),        \n",
    "    15,                                     \n",
    "    (frame_width, frame_height)             \n",
    ")\n",
    "\n",
    "while True:\n",
    "    use_screen = False  # True -> знімок екрану, False -> відео\n",
    "\n",
    "    if use_screen:\n",
    "        window_name = \"Screen Detection\"\n",
    "    else:\n",
    "        window_name = \"Video Detection\"\n",
    "\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(window_name, 960, 540)\n",
    "\n",
    "    if use_screen:\n",
    "        screenshot = pyautogui.screenshot()\n",
    "        frame = np.array(screenshot)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    else:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # відео закінчилося\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "    # оновлюємо результати тільки кожен n-й кадр\n",
    "    if frame_idx % process_every_n == 0:\n",
    "        results = model(frame, imgsz=640, stream=False)\n",
    "        last_results = results\n",
    "\n",
    "    # малюємо бокси тільки для потрібних класів\n",
    "    if last_results is not None:\n",
    "        for r in last_results:\n",
    "            boxes = r.boxes\n",
    "            for box in boxes:\n",
    "                cls_id = int(box.cls[0])\n",
    "                cls_name = model.names[cls_id] # отримуємо ім'я класу\n",
    "                if cls_name in classNames:     # перевіряємо, чи потрібний клас\n",
    "                    x1, y1, x2, y2 = box.xyxy[0]\n",
    "                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                    confidence = math.ceil((box.conf[0]*100))/100\n",
    "                    \n",
    "                    # малюємо прямокутник\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 255), 2)\n",
    "                    # підпис з назвою класу і confidence\n",
    "                    cv2.putText(frame, f\"{cls_name} {confidence}\", (x1, y1-10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,0,0), 2)\n",
    "\n",
    "    cv2.imshow(\"Screen Detection\", frame)\n",
    "    out.write(frame)  # записуємо кадр у файл\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# завершення запису\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42ebb4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 9 persons, 2 cups, 113.2ms\n",
      "Speed: 30.8ms preprocess, 113.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bottle, 2 cups, 113.9ms\n",
      "Speed: 3.2ms preprocess, 113.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 103.5ms\n",
      "Speed: 4.1ms preprocess, 103.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bottle, 2 cups, 103.2ms\n",
      "Speed: 3.3ms preprocess, 103.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 bottle, 1 cup, 104.5ms\n",
      "Speed: 3.0ms preprocess, 104.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 bottle, 1 cup, 104.9ms\n",
      "Speed: 3.6ms preprocess, 104.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bottle, 1 cup, 1 chair, 105.1ms\n",
      "Speed: 3.0ms preprocess, 105.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bottle, 1 cup, 119.0ms\n",
      "Speed: 3.8ms preprocess, 119.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 bottle, 1 cup, 2 chairs, 107.0ms\n",
      "Speed: 3.6ms preprocess, 107.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 cup, 1 chair, 139.4ms\n",
      "Speed: 5.3ms preprocess, 139.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2 cups, 1 chair, 104.2ms\n",
      "Speed: 3.7ms preprocess, 104.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 3 cups, 130.9ms\n",
      "Speed: 3.3ms preprocess, 130.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bottle, 3 cups, 1 chair, 163.2ms\n",
      "Speed: 3.1ms preprocess, 163.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bottle, 2 cups, 131.4ms\n",
      "Speed: 5.3ms preprocess, 131.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 bottle, 1 cup, 151.8ms\n",
      "Speed: 4.9ms preprocess, 151.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 2 chairs, 128.1ms\n",
      "Speed: 3.4ms preprocess, 128.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bottle, 2 cups, 139.9ms\n",
      "Speed: 3.6ms preprocess, 139.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 102.8ms\n",
      "Speed: 4.7ms preprocess, 102.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bottle, 2 cups, 111.0ms\n",
      "Speed: 3.6ms preprocess, 111.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 140.1ms\n",
      "Speed: 3.5ms preprocess, 140.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 137.3ms\n",
      "Speed: 3.4ms preprocess, 137.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 bottle, 2 cups, 1 chair, 113.0ms\n",
      "Speed: 3.2ms preprocess, 113.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 bottle, 2 cups, 1 potted plant, 128.7ms\n",
      "Speed: 3.6ms preprocess, 128.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 bottle, 3 cups, 2 chairs, 123.3ms\n",
      "Speed: 3.8ms preprocess, 123.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import hashlib\n",
    "\n",
    "# ---------------------------\n",
    "# Параметри\n",
    "# ---------------------------\n",
    "video_path = \"C:/Users/Angelika/Videos/Captures/coffee.mp4\"\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "classNames = [\"person\"]\n",
    "tracker = DeepSort(max_age=30)\n",
    "\n",
    "frame_idx = 0\n",
    "process_every_n = 3\n",
    "\n",
    "# Папка для збереження фото\n",
    "save_dir = \"saved_people\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Зберігаємо позиції між кадрами\n",
    "last_positions = {}\n",
    "movement_threshold = 20\n",
    "\n",
    "# Щоб уникати дублів\n",
    "saved_hashes = set()\n",
    "\n",
    "# ---------------------------\n",
    "# Відкриваємо відео\n",
    "# ---------------------------\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "cv2.namedWindow(\"Video Detection\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Video Detection\", 960, 540)\n",
    "\n",
    "def non_max_suppression(boxes, scores, iou_threshold=0.5):\n",
    "    \"\"\"Проста NMS для уникнення накладених рамок\"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    boxes = np.array(boxes)\n",
    "    scores = np.array(scores)\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,0] + boxes[:,2]\n",
    "    y2 = boxes[:,1] + boxes[:,3]\n",
    "\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "    order = scores.argsort()[::-1]\n",
    "    keep = []\n",
    "\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "        w = np.maximum(0.0, xx2 - xx1)\n",
    "        h = np.maximum(0.0, yy2 - yy1)\n",
    "        inter = w * h\n",
    "        iou = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "        inds = np.where(iou <= iou_threshold)[0]\n",
    "        order = order[inds + 1]\n",
    "    return keep\n",
    "\n",
    "def save_unique_image(crop, prefix=\"person\"):\n",
    "    \"\"\"Зберігає унікальне фото, щоб уникнути дублів\"\"\"\n",
    "    if crop.size == 0:  # перевірка, що зображення не пусте\n",
    "        return\n",
    "    img_bytes = cv2.imencode('.jpg', crop)[1].tobytes()\n",
    "    img_hash = hashlib.md5(img_bytes).hexdigest()\n",
    "    if img_hash not in saved_hashes:\n",
    "        filename = f\"{save_dir}/{prefix}_{img_hash}.jpg\"\n",
    "        cv2.imwrite(filename, crop)\n",
    "        saved_hashes.add(img_hash)\n",
    "\n",
    "# ---------------------------\n",
    "# Основний цикл обробки\n",
    "# ---------------------------\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_idx += 1\n",
    "\n",
    "    if frame_idx % process_every_n != 0:\n",
    "        continue\n",
    "\n",
    "    results = model(frame, imgsz=640, stream=False)\n",
    "\n",
    "    detections = []\n",
    "    scores = []\n",
    "\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            cls_name = model.names[cls_id]\n",
    "            if cls_name in classNames:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                conf = float(box.conf[0])\n",
    "                detections.append([x1, y1, x2 - x1, y2 - y1])\n",
    "                scores.append(conf)\n",
    "\n",
    "    keep_idx = non_max_suppression(detections, scores, iou_threshold=0.5)\n",
    "    detections_nms = [detections[i] for i in keep_idx]\n",
    "    scores_nms = [scores[i] for i in keep_idx]\n",
    "    labels_nms = [classNames[0]] * len(detections_nms)\n",
    "\n",
    "    tracks = tracker.update_tracks(\n",
    "        list(zip(detections_nms, scores_nms, labels_nms)), frame=frame\n",
    "    )\n",
    "\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        track_id = track.track_id\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "        cx, cy = (x1 + x2)//2, (y1 + y2)//2\n",
    "\n",
    "        # Малюємо рамку та ID\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "        cv2.putText(frame, f\"ID {track_id}\", (x1, y1-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "        # Перевірка руху\n",
    "        if track_id not in last_positions:\n",
    "            last_positions[track_id] = (cx, cy)\n",
    "        else:\n",
    "            last_x, last_y = last_positions[track_id]\n",
    "            dx, dy = abs(cx - last_x), abs(cy - last_y)\n",
    "            if dx > movement_threshold or dy > movement_threshold:\n",
    "                last_positions[track_id] = (cx, cy)\n",
    "                # Коригуємо координати, щоб не виходили за межі кадру\n",
    "                x1c, y1c = max(0, x1), max(0, y1)\n",
    "                x2c, y2c = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "                if x2c > x1c and y2c > y1c:\n",
    "                    person_crop = frame[y1c:y2c, x1c:x2c]\n",
    "                    save_unique_image(person_crop, prefix=f\"person_{track_id}\")\n",
    "\n",
    "    cv2.imshow(\"Video Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1cb7c274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 10 persons, 1 bottle, 1 cup, 164.9ms\n",
      "Speed: 35.3ms preprocess, 164.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 1 cup, 115.2ms\n",
      "Speed: 3.7ms preprocess, 115.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 cup, 210.2ms\n",
      "Speed: 6.0ms preprocess, 210.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bottle, 1 cup, 144.6ms\n",
      "Speed: 4.2ms preprocess, 144.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bottle, 1 cup, 114.9ms\n",
      "Speed: 3.9ms preprocess, 114.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bottle, 1 cup, 128.1ms\n",
      "Speed: 3.2ms preprocess, 128.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bottle, 1 cup, 133.1ms\n",
      "Speed: 4.0ms preprocess, 133.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bottle, 1 cup, 358.1ms\n",
      "Speed: 4.3ms preprocess, 358.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 1 cup, 126.6ms\n",
      "Speed: 4.1ms preprocess, 126.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 1 cup, 310.1ms\n",
      "Speed: 5.6ms preprocess, 310.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ---------------------------\n",
    "# Параметри\n",
    "# ---------------------------\n",
    "video_path = \"C:/Users/Angelika/Videos/Captures/coffee.mp4\"\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "classNames = [\"person\"]\n",
    "min_conf = 0.5          # мінімальна confidence для боксу\n",
    "min_box_size = 20       # мінімальний розмір боксу для людини\n",
    "track_history = defaultdict(list)  # зберігаємо координати центрів\n",
    "\n",
    "# ---------------------------\n",
    "# Відкриваємо відео\n",
    "# ---------------------------\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "cv2.namedWindow(\"YOLOv8 Tracking\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"YOLOv8 Tracking\", 960, 540)\n",
    "\n",
    "frame_idx = 0\n",
    "process_every_n = 1  # обробляти кожен 2-й кадр\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "    if frame_idx % process_every_n == 0:\n",
    "        # детекція і трекінг людей\n",
    "        results = model.track(frame, persist=True, imgsz=640)\n",
    "\n",
    "        # обробка тільки першого результату\n",
    "        r = results[0]\n",
    "        boxes = r.boxes\n",
    "        if boxes is not None:\n",
    "            for box in boxes:\n",
    "                cls_id = int(box.cls[0])\n",
    "                cls_name = model.names[cls_id]\n",
    "                conf = float(box.conf[0])\n",
    "                if cls_name in classNames and conf >= min_conf:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    w, h = x2 - x1, y2 - y1\n",
    "                    if w >= min_box_size and h >= min_box_size:\n",
    "                        track_id = int(box.id[0])\n",
    "\n",
    "                        # малюємо рамку\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 5)\n",
    "                        cv2.putText(frame, f\"ID {track_id}\", (x1, y1-15),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 2.0, (0, 255, 0), 5)\n",
    "\n",
    "                        # зберігаємо центр боксу для відстеження\n",
    "                        cx, cy = (x1+x2)//2, (y1+y2)//2\n",
    "                        track_history[track_id].append((cx, cy))\n",
    "                        if len(track_history[track_id]) > 50:\n",
    "                            track_history[track_id].pop(0)\n",
    "\n",
    "                        # малюємо шлях треку\n",
    "                        points = np.array(track_history[track_id], np.int32).reshape((-1,1,2))\n",
    "                        cv2.polylines(frame, [points], False, (200,200,200), 2)\n",
    "\n",
    "    cv2.imshow(\"YOLOv8 Tracking\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "67240f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 9 persons, 1 cup, 250.9ms\n",
      "Speed: 4.2ms preprocess, 250.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 cup, 192.5ms\n",
      "Speed: 6.2ms preprocess, 192.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "# ---------------------------\n",
    "# Параметри\n",
    "# ---------------------------\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "classNames = [\"person\", \"cat\", \"dog\"]\n",
    "tracker = DeepSort(max_age=30)\n",
    "\n",
    "frame_idx = 0\n",
    "process_every_n = 3   # обробляти кожен 3-й кадр\n",
    "\n",
    "video_path = \"C:/Users/Angelika/Videos/Captures/coffee_shop.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Налаштування запису відео\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "out = cv2.VideoWriter(\n",
    "    'screen_detection.mp4',\n",
    "    cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "    15,\n",
    "    (screen_width, screen_height)\n",
    ")\n",
    "\n",
    "# Папка для збереження людей\n",
    "save_dir = \"saved_people\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Для уникнення дублів\n",
    "saved_hashes = set()\n",
    "# Для обмеження кількості кадрів на одну людину\n",
    "frames_per_person = {}\n",
    "\n",
    "# ---------------------------\n",
    "# Функція збереження унікального кадру людини\n",
    "# ---------------------------\n",
    "def save_person_crop(track_id, crop):\n",
    "    if crop.size == 0:\n",
    "        return\n",
    "    img_bytes = cv2.imencode('.jpg', crop)[1].tobytes()\n",
    "    img_hash = hashlib.md5(img_bytes).hexdigest()\n",
    "    if img_hash in saved_hashes:\n",
    "        return\n",
    "    saved_hashes.add(img_hash)\n",
    "\n",
    "    person_folder = os.path.join(save_dir, f\"person_{track_id}\")\n",
    "    os.makedirs(person_folder, exist_ok=True)\n",
    "\n",
    "    count = frames_per_person.get(track_id, 0)\n",
    "    if count >= 5:\n",
    "        return\n",
    "    frames_per_person[track_id] = count + 1\n",
    "\n",
    "    filename = os.path.join(person_folder, f\"{count+1}_{img_hash}.jpg\")\n",
    "    cv2.imwrite(filename, crop)\n",
    "\n",
    "# ---------------------------\n",
    "# Основний цикл\n",
    "# ---------------------------\n",
    "while True:\n",
    "    use_screen = False  # True -> знімок екрану, False -> відео\n",
    "    window_name = \"Screen Detection\" if use_screen else \"Video Detection\"\n",
    "\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(window_name, 960, 540)\n",
    "\n",
    "    # Отримуємо кадр\n",
    "    if use_screen:\n",
    "        screenshot = pyautogui.screenshot()\n",
    "        frame = cv2.cvtColor(np.array(screenshot), cv2.COLOR_RGB2BGR)\n",
    "    else:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "    if frame is None or frame.size == 0:\n",
    "        continue  # пропускаємо пустий кадр\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "    # Оновлюємо результати тільки кожен n-й кадр\n",
    "    if frame_idx % process_every_n == 0:\n",
    "        results = model(frame, imgsz=640, stream=False)\n",
    "\n",
    "    # --- Малюємо бокси тільки для потрібних класів ---\n",
    "    if results is not None:\n",
    "        detections = []\n",
    "        scores = []\n",
    "        labels = []\n",
    "\n",
    "        for r in results:\n",
    "            for box in r.boxes:\n",
    "                cls_id = int(box.cls[0])\n",
    "                cls_name = model.names[cls_id]\n",
    "                if cls_name in classNames:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    conf = float(box.conf[0])\n",
    "                    w, h = x2 - x1, y2 - y1\n",
    "                    if w > 0 and h > 0:  # перевірка валідності\n",
    "                        detections.append([x1, y1, w, h])\n",
    "                        scores.append(conf)\n",
    "                        labels.append(cls_name)\n",
    "\n",
    "        # --- Безпечний виклик DeepSORT ---\n",
    "        tracks = []\n",
    "        if len(detections) > 0:\n",
    "            try:\n",
    "                tracks = tracker.update_tracks(\n",
    "                    list(zip(detections, scores, labels)),\n",
    "                    frame=frame\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"[DeepSORT] Помилка при трекінгу: {e}\")\n",
    "\n",
    "        # --- Малюємо та зберігаємо ---\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            track_id = track.track_id\n",
    "            x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "\n",
    "            # Малюємо рамку та ID\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 255), 2)\n",
    "            cv2.putText(frame, f\"ID {track_id}\", (x1, y1-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,0,0), 2)\n",
    "\n",
    "            # Зберігаємо кадри людини\n",
    "            x1c, y1c = max(0, x1), max(0, y1)\n",
    "            x2c, y2c = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "            if x2c > x1c and y2c > y1c:\n",
    "                crop = frame[y1c:y2c, x1c:x2c]\n",
    "                if crop.shape[0] >= 20 and crop.shape[1] >= 20:  # мінімальний розмір\n",
    "                    save_person_crop(track_id, crop)\n",
    "\n",
    "    cv2.imshow(window_name, frame)\n",
    "    out.write(frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6020925e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 9 persons, 1 cup, 349.0ms\n",
      "Speed: 34.4ms preprocess, 349.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 cup, 234.3ms\n",
      "Speed: 3.5ms preprocess, 234.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 cup, 238.7ms\n",
      "Speed: 6.9ms preprocess, 238.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# ---------------------------\n",
    "# Параметри\n",
    "# ---------------------------\n",
    "video_path = \"C:/Users/Angelika/Videos/Captures/coffee_shop.mp4\"\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "classNames = [\"person\"]  # тільки люди\n",
    "tracker = DeepSort(max_age=30)\n",
    "\n",
    "frame_idx = 0\n",
    "process_every_n = 3\n",
    "last_results = None\n",
    "\n",
    "# Папка для збереження фото\n",
    "os.makedirs(\"detections\", exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Відкриваємо відео\n",
    "# ---------------------------\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "cv2.namedWindow(\"Video Detection\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Video Detection\", 960, 540)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "    # оновлюємо тільки кожен n-й кадр\n",
    "    if frame_idx % process_every_n == 0:\n",
    "        results = model(frame, imgsz=640, stream=False)\n",
    "        last_results = results\n",
    "\n",
    "    if last_results is not None:\n",
    "        detections = []\n",
    "        for r in last_results:\n",
    "            for box in r.boxes:\n",
    "                cls_id = int(box.cls[0])\n",
    "                cls_name = model.names[cls_id]\n",
    "                if cls_name in classNames:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    conf = float(box.conf[0])\n",
    "                    detections.append(([x1, y1, x2 - x1, y2 - y1], conf, cls_name))\n",
    "\n",
    "        # запускаємо DeepSORT\n",
    "        tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            track_id = track.track_id\n",
    "            ltrb = track.to_ltrb()\n",
    "            x1, y1, x2, y2 = map(int, ltrb)\n",
    "\n",
    "            # малюємо прямокутник + айді\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255,0,255), 2)\n",
    "            cv2.putText(frame, f\"ID {track_id}\", (x1, y1-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,0,0), 2)\n",
    "\n",
    "            # --- збереження фото ---\n",
    "            person_crop = frame[y1:y2, x1:x2]\n",
    "            if person_crop.size > 0:\n",
    "                person_dir = f\"detections/ID{track_id}\"\n",
    "                os.makedirs(person_dir, exist_ok=True)\n",
    "                save_path = f\"{person_dir}/frame{frame_idx}.jpg\"\n",
    "                cv2.imwrite(save_path, person_crop)\n",
    "\n",
    "    cv2.imshow(\"Video Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "78874d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 9 persons, 1 cup, 158.0ms\n",
      "Speed: 3.5ms preprocess, 158.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 cup, 178.6ms\n",
      "Speed: 7.4ms preprocess, 178.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 cup, 219.7ms\n",
      "Speed: 4.6ms preprocess, 219.7ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 cups, 187.2ms\n",
      "Speed: 32.2ms preprocess, 187.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 321.0ms\n",
      "Speed: 4.9ms preprocess, 321.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 1 cup, 204.7ms\n",
      "Speed: 5.7ms preprocess, 204.7ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 174.0ms\n",
      "Speed: 4.8ms preprocess, 174.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bottle, 1 cup, 171.5ms\n",
      "Speed: 5.4ms preprocess, 171.5ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 1 chair, 175.7ms\n",
      "Speed: 5.5ms preprocess, 175.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 160.4ms\n",
      "Speed: 4.5ms preprocess, 160.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 bottle, 2 cups, 184.3ms\n",
      "Speed: 7.3ms preprocess, 184.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import hashlib\n",
    "\n",
    "# ---------------------------\n",
    "# Параметри\n",
    "# ---------------------------\n",
    "video_path = \"C:/Users/Angelika/Videos/Captures/coffee_shop.mp4\"\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "classNames = [\"person\"]\n",
    "tracker = DeepSort(max_age=30)\n",
    "\n",
    "frame_idx = 0\n",
    "process_every_n = 3\n",
    "\n",
    "# Папка для збереження фото\n",
    "save_dir = \"detections\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Для відстеження попереднього стану людини\n",
    "last_state = {}  # {track_id: (x, y, w, h)}\n",
    "max_photos_per_person = 10\n",
    "\n",
    "# ---------------------------\n",
    "# Відкриваємо відео\n",
    "# ---------------------------\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "cv2.namedWindow(\"Video Detection\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Video Detection\", 960, 540)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "    # оновлюємо тільки кожен n-й кадр\n",
    "    if frame_idx % process_every_n == 0:\n",
    "        results = model(frame, imgsz=640, stream=False)\n",
    "\n",
    "    if results is not None:\n",
    "        detections = []\n",
    "        for r in results:\n",
    "            for box in r.boxes:\n",
    "                cls_id = int(box.cls[0])\n",
    "                cls_name = model.names[cls_id]\n",
    "                if cls_name in classNames:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    conf = float(box.conf[0])\n",
    "                    detections.append(([x1, y1, x2 - x1, y2 - y1], conf, cls_name))\n",
    "\n",
    "        # запускаємо DeepSORT\n",
    "        tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "        for track in tracks:\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "\n",
    "            track_id = track.track_id\n",
    "            x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "\n",
    "            # Малюємо прямокутник + ID\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 255), 2)\n",
    "            cv2.putText(frame, f\"ID {track_id}\", (x1, y1-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "\n",
    "            # --- збереження фото тільки при зміні стану ---\n",
    "            person_crop = frame[y1:y2, x1:x2]\n",
    "            if person_crop.size == 0:\n",
    "                continue\n",
    "\n",
    "            person_dir = os.path.join(save_dir, f\"ID{track_id}\")\n",
    "            os.makedirs(person_dir, exist_ok=True)\n",
    "\n",
    "            # хеш кадру, щоб уникнути дублів\n",
    "            img_bytes = cv2.imencode('.jpg', person_crop)[1].tobytes()\n",
    "            img_hash = hashlib.md5(img_bytes).hexdigest()\n",
    "\n",
    "            # перевірка на попередній стан\n",
    "            prev = last_state.get(track_id)\n",
    "            state_changed = True\n",
    "            if prev:\n",
    "                px, py, pw, ph = prev\n",
    "                # якщо зміна менша ніж 10% по ширині/висоті та положенню — не зберігаємо\n",
    "                if abs(px - x1) < 0.1*pw and abs(py - y1) < 0.1*ph and abs(pw - w) < 0.1*pw and abs(ph - h) < 0.1*ph:\n",
    "                    state_changed = False\n",
    "\n",
    "            # обмеження на 10 фото\n",
    "            existing_photos = len(os.listdir(person_dir))\n",
    "            if state_changed and existing_photos < max_photos_per_person:\n",
    "                save_path = os.path.join(person_dir, f\"{existing_photos+1}_{img_hash}.jpg\")\n",
    "                cv2.imwrite(save_path, person_crop)\n",
    "                last_state[track_id] = (x1, y1, w, h)\n",
    "\n",
    "    cv2.imshow(\"Video Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "23cb604f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 9 persons, 1 cup, 261.6ms\n",
      "Speed: 36.8ms preprocess, 261.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 cup, 167.8ms\n",
      "Speed: 3.4ms preprocess, 167.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 cup, 163.9ms\n",
      "Speed: 4.2ms preprocess, 163.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Track' object has no attribute 'curr_feat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m track_id \u001b[38;5;241m=\u001b[39m track\u001b[38;5;241m.\u001b[39mtrack_id\n\u001b[0;32m     63\u001b[0m x1, y1, x2, y2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, track\u001b[38;5;241m.\u001b[39mto_ltrb())\n\u001b[1;32m---> 64\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mtrack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurr_feat\u001b[49m  \u001b[38;5;66;03m# поточний embedding треку\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# малюємо прямокутник + айді\u001b[39;00m\n\u001b[0;32m     67\u001b[0m cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (x1, y1), (x2, y2), (\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Track' object has no attribute 'curr_feat'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# ---------------------------\n",
    "# Параметри\n",
    "# ---------------------------\n",
    "video_path = \"C:/Users/Angelika/Videos/Captures/coffee_shop.mp4\"\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "classNames = [\"person\"]  # тільки люди\n",
    "tracker = DeepSort(max_age=30)\n",
    "\n",
    "frame_idx = 0\n",
    "process_every_n = 3\n",
    "\n",
    "# Папка для збереження фото\n",
    "save_dir = \"detections\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Словник embeddings для порівняння\n",
    "person_embeddings = {}  # {track_id: [embedding1, embedding2,...]}\n",
    "\n",
    "# ---------------------------\n",
    "# Відкриваємо відео\n",
    "# ---------------------------\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "cv2.namedWindow(\"Video Detection\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Video Detection\", 960, 540)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "    # оновлюємо тільки кожен n-й кадр\n",
    "    if frame_idx % process_every_n != 0:\n",
    "        continue\n",
    "\n",
    "    results = model(frame, imgsz=640, stream=False)\n",
    "\n",
    "    detections = []\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            cls_name = model.names[cls_id]\n",
    "            if cls_name in classNames:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                conf = float(box.conf[0])\n",
    "                detections.append(([x1, y1, x2 - x1, y2 - y1], conf, cls_name))\n",
    "\n",
    "    # --- DeepSORT трекер ---\n",
    "    tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        track_id = track.track_id\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "        embedding = track.curr_feat  # поточний embedding треку\n",
    "\n",
    "        # малюємо прямокутник + айді\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (255,0,255), 2)\n",
    "        cv2.putText(frame, f\"ID {track_id}\", (x1, y1-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,0,0), 2)\n",
    "\n",
    "        # --- перевірка на нову позу ---\n",
    "        save_photo = False\n",
    "        person_crop = frame[y1:y2, x1:x2]\n",
    "        if person_crop.size > 0:\n",
    "            crop_rgb = cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB)\n",
    "            embedding = tracker.embedder.predict([crop_rgb])[0]  # отримуємо embedding\n",
    "\n",
    "            if track_id not in person_embeddings:\n",
    "                person_embeddings[track_id] = []\n",
    "                save_photo = True\n",
    "            else:\n",
    "                # порівнюємо з усіма попередніми embeddings\n",
    "                similarities = [1 - cosine(embedding, e) for e in person_embeddings[track_id]]\n",
    "                if len(similarities) == 0 or max(similarities) < 0.7:  # нова поза\n",
    "                    save_photo = True\n",
    "\n",
    "            # --- збереження фото ---\n",
    "            if save_photo:\n",
    "                person_dir = os.path.join(save_dir, f\"ID{track_id}\")\n",
    "                os.makedirs(person_dir, exist_ok=True)\n",
    "                existing = len(os.listdir(person_dir))\n",
    "                if existing < 10:\n",
    "                    save_path = os.path.join(person_dir, f\"frame{frame_idx}.jpg\")\n",
    "                    cv2.imwrite(save_path, person_crop)\n",
    "                    person_embeddings[track_id].append(embedding)\n",
    "\n",
    "\n",
    "    cv2.imshow(\"Video Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
